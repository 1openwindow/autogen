{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Group Chat\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "This notebook is modified based on https://github.com/microsoft/FLAML/blob/4ea686af5c3e8ff24d9076a7a626c8b28ab5b1d7/notebook/autogen_multiagent_roleplay_chat.ipynb\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install pyautogen~=0.2.0b4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "# config_list_gpt35 = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": {\n",
    "#             \"gpt-3.5-turbo\",\n",
    "#             \"gpt-3.5-turbo-16k\",\n",
    "#             \"gpt-3.5-turbo-0301\",\n",
    "#             \"chatgpt-35-turbo-0301\",\n",
    "#             \"gpt-35-turbo-v0301\",\n",
    "#         },\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_openai_utils.ipynb) for full code examples of the different methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\"config_list\": config_list_gpt4, \"cache_seed\": 42}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "   name=\"User_proxy\",\n",
    "   system_message=\"A human admin.\",\n",
    "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "   human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To find the latest paper about GPT-4 on arXiv, we can use the arXiv API to query for papers that mention \"GPT-4\" in their metadata. We will write a Python script that queries the API and prints out the title, authors, summary, and publication date of the most recent paper related to GPT-4. After obtaining the paper details, I will analyze the abstract to deduce its potential applications in software.\n",
      "\n",
      "Let's start by writing the Python script to query the arXiv API. Please run the following code:\n",
      "\n",
      "```python\n",
      "# filename: arxiv_query.py\n",
      "import requests\n",
      "from feedparser import parse\n",
      "\n",
      "# Define the query parameters\n",
      "query_params = {\n",
      "    'search_query': 'all:GPT-4',\n",
      "    'sortBy': 'submittedDate',\n",
      "    'sortOrder': 'descending',\n",
      "    'max_results': 1  # We only need the latest paper\n",
      "}\n",
      "\n",
      "# Base URL for the arXiv API\n",
      "arxiv_api_url = 'http://export.arxiv.org/api/query'\n",
      "\n",
      "# Perform the API request\n",
      "response = requests.get(arxiv_api_url, params=query_params)\n",
      "\n",
      "# Check if the request was successful\n",
      "if response.status_code == 200:\n",
      "    # Parse the feed\n",
      "    feed = parse(response.content)\n",
      "    \n",
      "    if feed.entries:\n",
      "        # Get the latest entry\n",
      "        latest_paper = feed.entries[0]\n",
      "        \n",
      "        # Extract the details we need\n",
      "        title = latest_paper.title\n",
      "        authors = ', '.join(author.name for author in latest_paper.authors)\n",
      "        summary = latest_paper.summary\n",
      "        published = latest_paper.published\n",
      "        \n",
      "        # Print the details\n",
      "        print(f\"Title: {title}\")\n",
      "        print(f\"Authors: {authors}\")\n",
      "        print(f\"Summary: {summary}\")\n",
      "        print(f\"Published Date: {published}\")\n",
      "    else:\n",
      "        print(\"No papers related to GPT-4 found.\")\n",
      "else:\n",
      "    print(f\"Failed to query arXiv API. Status Code: {response.status_code}\")\n",
      "```\n",
      "\n",
      "Please save the above script as `arxiv_query.py` and run it. It should output the details of the most recent paper related to GPT-4 on arXiv. If there's an error or if the output doesn't show the latest paper's details, please let me know the error message or the output you receive.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"arxiv_query.py\", line 3, in <module>\n",
      "    from feedparser import parse\n",
      "ModuleNotFoundError: No module named 'feedparser'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems that the `feedparser` module is not installed on your system. You will need to install this module before you can run the script. Please run the following command to install the `feedparser` module:\n",
      "\n",
      "```sh\n",
      "pip install feedparser\n",
      "```\n",
      "\n",
      "After installing `feedparser`, please try running the `arxiv_query.py` script again. If you face any further issues, please let me know.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/81.3 kB\u001b[0m \u001b[31m270.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/81.3 kB\u001b[0m \u001b[31m273.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/81.3 kB\u001b[0m \u001b[31m273.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m61.4/81.3 kB\u001b[0m \u001b[31m283.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/81.3 kB\u001b[0m \u001b[31m274.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m308.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=224e1d6722c49f3dc93faa79b601fee4f7bd92d80384afa9fd1a603771abe74a\n",
      "  Stored in directory: /home/vscode/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: feedparser in /home/vscode/.local/lib/python3.10/site-packages (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in /home/vscode/.local/lib/python3.10/site-packages (from feedparser) (1.0.0)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Great, it looks like the `feedparser` module has been installed successfully. Now that the required module is available, please run the `arxiv_query.py` script again to query the latest GPT-4 paper on arXiv. Here is the script again for your convenience. Save it as `arxiv_query.py` if you haven't already, and execute it.\n",
      "\n",
      "```python\n",
      "# filename: arxiv_query.py\n",
      "import requests\n",
      "from feedparser import parse\n",
      "\n",
      "# Define the query parameters\n",
      "query_params = {\n",
      "    'search_query': 'all:GPT-4',\n",
      "    'sortBy': 'submittedDate',\n",
      "    'sortOrder': 'descending',\n",
      "    'max_results': 1  # We only need the latest paper\n",
      "}\n",
      "\n",
      "# Base URL for the arXiv API\n",
      "arxiv_api_url = 'http://export.arxiv.org/api/query'\n",
      "\n",
      "# Perform the API request\n",
      "response = requests.get(arxiv_api_url, params=query_params)\n",
      "\n",
      "# Check if the request was successful\n",
      "if response.status_code == 200:\n",
      "    # Parse the feed\n",
      "    feed = parse(response.content)\n",
      "    \n",
      "    if feed.entries:\n",
      "        # Get the latest entry\n",
      "        latest_paper = feed.entries[0]\n",
      "        \n",
      "        # Extract the details we need\n",
      "        title = latest_paper.title\n",
      "        authors = ', '.join(author.name for author in latest_paper.authors)\n",
      "        summary = latest_paper.summary\n",
      "        published = latest_paper.published\n",
      "        \n",
      "        # Print the details\n",
      "        print(f\"Title: {title}\")\n",
      "        print(f\"Authors: {authors}\")\n",
      "        print(f\"Summary: {summary}\")\n",
      "        print(f\"Published Date: {published}\")\n",
      "    else:\n",
      "        print(\"No papers related to GPT-4 found.\")\n",
      "else:\n",
      "    print(f\"Failed to query arXiv API. Status Code: {response.status_code}\")\n",
      "```\n",
      "\n",
      "Run the script and let me know the output.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: From Text to Motion: Grounding GPT-4 in a Humanoid Robot \"Alter3\"\n",
      "Authors: Takahide Yoshida, Atsushi Masumori, Takashi Ikegami\n",
      "Summary: We report the development of Alter3, a humanoid robot capable of generating\n",
      "spontaneous motion using a Large Language Model (LLM), specifically GPT-4. This\n",
      "achievement was realized by integrating GPT-4 into our proprietary android,\n",
      "Alter3, thereby effectively grounding the LLM with Alter's bodily movement.\n",
      "Typically, low-level robot control is hardware-dependent and falls outside the\n",
      "scope of LLM corpora, presenting challenges for direct LLM-based robot control.\n",
      "However, in the case of humanoid robots like Alter3, direct control is feasible\n",
      "by mapping the linguistic expressions of human actions onto the robot's body\n",
      "through program code. Remarkably, this approach enables Alter3 to adopt various\n",
      "poses, such as a 'selfie' stance or 'pretending to be a ghost,' and generate\n",
      "sequences of actions over time without explicit programming for each body part.\n",
      "This demonstrates the robot's zero-shot learning capabilities. Additionally,\n",
      "verbal feedback can adjust poses, obviating the need for fine-tuning. A video\n",
      "of Alter3's generated motions is available at\n",
      "https://tnoinkwms.github.io/ALTER-LLM/\n",
      "Published Date: 2023-12-11T17:57:11Z\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the summary provided, the paper titled \"From Text to Motion: Grounding GPT-4 in a Humanoid Robot 'Alter3'\" describes an innovative application of the GPT-4 language model in the domain of robotics.\n",
      "\n",
      "Here are some potential applications in software:\n",
      "\n",
      "1. **Robotics Software Development:**\n",
      "   This research has demonstrated that LLMs like GPT-4 can be used to control humanoid robots without the need for explicit programming of each body part. In software, this approach can be further developed to create adaptable and advanced control systems for humanoid robots across various industries, including entertainment, assistance to the elderly or individuals with disabilities, and even education where robots could interact and engage with students.\n",
      "\n",
      "2. **Natural Language Interface for Robotics Control:**\n",
      "   The work suggests that software interfaces could be created to allow operators to control robots through natural language commands. This could lead to increased accessibility and a more intuitive way of operating complex robotic systems, making robots more accessible to those without programming expertise.\n",
      "\n",
      "3. **Zero-Shot Learning and Adaptation:**\n",
      "   The application in the paper indicates that GPT-4 can enable robots to perform tasks they weren't explicitly programmed for (zero-shot learning). Software platforms can leverage this capability to reduce the amount of time and resources needed to train robots for specific tasks.\n",
      "\n",
      "4. **Entertainment and Creative Industries:**\n",
      "   GPT-4's application here could translate into software that generates creative content or performances in real-time. This could be used in theme parks, art installations, or as part of live performances where a robot could participate as a performer, reacting to its environment or predefined scripts.\n",
      "\n",
      "5. **Interactive Feedback Systems:**\n",
      "   Incorporating verbal feedback to adjust a robot's actions implies that software could be created for interactive human-robot collaboration where adjustments and modifications to tasks can be made on the fly, based on spoken instructions.\n",
      "\n",
      "6. **Simulation and Modeling:**\n",
      "   Software that simulates human-robot interactions could be vastly improved by integrating GPT-4 to make these simulations more realistic. This could be useful in AI research, human factors engineering, and the design of ergonomic products.\n",
      "\n",
      "7. **Education and Research:**\n",
      "   The methodology provided by the integration of GPT-4 can serve as an educational tool, helping students and researchers understand the concepts of AI and robotics. Software could be designed for educational purposes, providing hands-on learning experiences through natural language interaction with robots.\n",
      "\n",
      "From the summary provided, the potential of GPT-4 in enhancing humanoid robots’ capabilities is evident. The implications for software development, particularly in the robotics and AI fields, are numerous and could lead to more intuitive, intelligent, and accessible robotic and AI systems.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")\n",
    "# type exit to terminate the chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
